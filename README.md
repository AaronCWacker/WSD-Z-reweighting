# WSD-Z-reweighting
The is the code repo for The ACL 2021 paper [Rare and zero-shot word sense disambiguation using Z-reweighting](https://github.com/suytingwan/WSD-Z-reweighting)

## Envs:
1. for bert-base

   python 3.7.6

   torch 1.2.0
   
   transformer 4.1.1

2. for bert-large

   python 3.8.8
   
   torch 1.7.1
   
   transformer 3.4.4
   
   pytorch_transformers 1.1.0 (for replicating original biencoder version)

## Data preparation
   Before running the experiments, download training data [Semcor](http://lcl.uniroma1.it/wsdeval/training-data).
   Also, the [unified evaluation framework for WSD](http://lcl.uniroma1.it/wsdeval/).
   ```
   mkdir data
   cd data
   wget http://lcl.uniroma1.it/wsdeval/data/WSD_Evaluation_Framework.zip
   unzip WSD_Evaluation_Framework.zip
   ```

## How to prepare weights for Z-reweighting strategy
   Before everything starts, firstly transform the xml format into csv
   ```
   cd ./preprocess
   python transform.py
   ```
   resulting semcor.csv file, similarily, applying to senseval2, senseval3, etc.

### Sort words by frequency order
    Get polysemy distribution and instance number for words and senses, set K value and calculate smoothed polysemy distribution
    ```
    cd ./preprocess
    python poly_power.py
    ``` 
    resulting semcor_sense_count.json,  semcor_synset_count.txt, semcor_polysemy_{K}.npy, where K = 50, 100, 200, 300, 400.
### use power law function to fit the polysemy distribution
    Set lammda and assign weight to training words in SemCor
    ```
    python power_law_fit.py
    ```
    This will first use the threadholds to group the words by one-decimal score generated by power-law fitting curve. According to groups, futher set gamma to adjust the weight for training words and generate weight file semcor_synset_weight_{K}_gamma.json, which will be used later in Z-reweighting strategy.
    
    One fitting curve before and after is:
    ![original](https://drive.google.com/drive/folders/1wbGuGSMbTnJ4tRaJfvaeMp5x6D7-D7lr)
    ![Fitted curve](https://drive.google.com/drive/folders/1wbGuGSMbTnJ4tRaJfvaeMp5x6D7-D7lr)
    
## How to run code
   For different strategies, the training scripts are:
   ```
   CUDA_VISIBLE_DEVICES=0,1 python biencoder_Z_reweighting.py --data-path ./data --postprocess-data-path ./preprocess --K 300 --gamma 2 --ckpt bert_base_Z_reweighting_300_2 --encoder-name bert-base --multigpu
   ```
   
   Our trained checkpoint for K=300, gamma=2 on Z-reweighting strategy can be downloaded at [model](https://drive.google.com/drive/folders/1afKaTf4mC3cE7rvC8rjJOa4_pFwtEY9v?usp=sharing). For easy evaluation:
   ```
   CUDA_VISIBLE_DEVICES=0,1 python biencoder_Z_reweighting.py --data-path ./data --postprocess-data-path ./preprocess --ckpt bert_base_Z_reweighting_300_2 --encoder-name bert-base --multigpu --split ALL --eval
   ```

## MCS/LCS analysis
   Use definition of MCS and LCS from WordNet, top1 ranked sense is MCS, others are LCS.
   change the input path and run:
   ```
   cd ./analysis
   python analyze_mcs_lcs.py
   ```
